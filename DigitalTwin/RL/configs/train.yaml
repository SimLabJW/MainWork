env:
  meters_per_pixel: 0.10
  max_steps: 800                # ⇧ 복잡 지형에서도 에피소드가 너무 빨리 끊기지 않도록 여유
  forward_step: 0.40
  turn_step_rad: 0.17453292519943295   # 10 deg
  robot_radius_m: 0.25

  # reward/behavior (v3)
  time_penalty: 0.01            # ⇩ 너무 급하게 종료로 몰지 않도록 완화
  progress_gain: 4.0
  rot_penalty: 0.01
  heading_gain: 0.30            # ⇧ 목표 방향 정렬을 더 강하게 유도 (속도제어 안정)
  goal_reach_dist: 0.50
  collision_penalty: 5.0        # ⇧ 충돌 억제 강화
  # map thresholds
  occ_thresh: 0.65
  free_thresh: 0.35

  seed: 42
  control_period_s: 0.25

  # ray settings (v3: 3개 레이)
  use_rays: true
  ray_angles_deg: [-30.0, 0.0, 30.0]
  ray_max_dist_m: 5.0           # ⇧ 근거리 회피만으로는 부족할 때 약간 더 멀리 본다
  near_obs_penalty: 1.2         # ⇧ 벽·장애물 근접 회피 가중

dqn:
  device: "cuda"
  learning_rate: 0.0001
  buffer_size: 500000           # ⇧ 더 다양한 경험 유지
  learning_starts: 10000        # ⇧ 초반 정책 불안정 완화
  batch_size: 256
  train_freq: 16
  gradient_steps: 4
  gamma: 0.99
  target_update_interval: 8000  # ⇩ 타깃 네트워크 조금 더 자주 동기화(10k→8k)
  exploration_initial_eps: 1.0
  exploration_final_eps: 0.05
  exploration_fraction: 0.15    # ⇩ 탐험 기간을 약간 줄여 수렴 가속 (롤아웃은 deterministic)
  verbose: 0

train:
  tb_log_dir: "./logs/dqn_explore/"
